{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "import jsonlines\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'ruby':DFG_ruby,\n",
    "    'go':DFG_go,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with jsonlines.open('synthesis_results_.jsonl') as f:\n",
    "    for i in f:\n",
    "        data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeX and GPT3.5 \n",
    "lang = 'python'\n",
    "alpha,beta,gamma,theta = 0.25,0.25,0.25,0.25\n",
    "references = [[i['label'].strip()] for i in data]\n",
    "hypothesis = [i['choices'][0]['text'].strip() for i in data]\n",
    "tokenized_hyps = [x.split() for x in hypothesis]\n",
    "tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "def make_weights(reference_tokens, key_word_list):\n",
    "    return {token:1 if token in key_word_list else 0.2 \\\n",
    "            for token in reference_tokens}\n",
    "tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "            for reference_tokens in reference] for reference in tokenized_refs]\n",
    "weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "code_bleu_score = alpha*ngram_match_score\\\n",
    "                + beta*weighted_ngram_match_score\\\n",
    "                + gamma*syntax_match_score\\\n",
    "                + theta*dataflow_match_score\n",
    "ref = {}\n",
    "hypo = {}\n",
    "count = 0\n",
    "em=0\n",
    "for i in data:\n",
    "    ref[count] = [i['label']]\n",
    "    hypo[count] = [i['choices'][0]['text'] if i['choices'][0]['text'].strip() is not '' else '.']\n",
    "    if ' '.join(ref[count][0].strip().split()) == ' '.join(hypo[count][0].strip().split()):\n",
    "        em+=1\n",
    "    count+=1\n",
    "print(round(100*em/len(data),2),round(100*syntax_match_score,2),round(100*dataflow_match_score,2),round(100*code_bleu_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT\n",
    "lang = 'python'\n",
    "alpha,beta,gamma,theta = 0.25,0.25,0.25,0.25\n",
    "references = [[i['label'].strip()] for i in data]\n",
    "hypothesis = [i['choices'][0]['message']['content'].strip() for i in data]\n",
    "tokenized_hyps = [x.split() for x in hypothesis]\n",
    "tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "def make_weights(reference_tokens, key_word_list):\n",
    "    return {token:1 if token in key_word_list else 0.2 \\\n",
    "            for token in reference_tokens}\n",
    "tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "            for reference_tokens in reference] for reference in tokenized_refs]\n",
    "weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "code_bleu_score = alpha*ngram_match_score\\\n",
    "                + beta*weighted_ngram_match_score\\\n",
    "                + gamma*syntax_match_score\\\n",
    "                + theta*dataflow_match_score\n",
    "ref = {}\n",
    "hypo = {}\n",
    "count = 0\n",
    "em=0\n",
    "for i in data:\n",
    "    ref[count] = [i['label']]\n",
    "    hypo[count] = [i['choices'][0]['message']['content'] if i['choices'][0]['message']['content'].strip() is not '' else '.']\n",
    "    if ' '.join(ref[count][0].strip().split()) == ' '.join(hypo[count][0].strip().split()):\n",
    "        em+=1\n",
    "    count+=1\n",
    "print(round(100*em/len(data),2),round(100*syntax_match_score,2),round(100*dataflow_match_score,2),round(100*code_bleu_score,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
